{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87605355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 14883 samples and 224128 outcomes>\n",
      "['c#', 'java', 'javascript', 'android', 'python', 'c++', 'php', 'jquery', '.net', 'ios', 'html', 'css', 'c', 'iphone', 'objective-c', 'ruby-on-rails', 'sql', 'asp.net', 'mysql', 'ruby', 'r', 'git', 'asp.net-mvc', 'linux', 'sql-server']\n",
      "(52418,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2          [sql, asp.net]\n",
       "4              [c#, .net]\n",
       "5                   [c++]\n",
       "6                  [.net]\n",
       "7            [sql-server]\n",
       "                ...      \n",
       "1262668             [c++]\n",
       "1262834             [c++]\n",
       "1262915          [python]\n",
       "1263065          [python]\n",
       "1263454             [c++]\n",
       "Name: Tags, Length: 52418, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import re\n",
    "import lxml.etree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(10**6)\n",
    "\n",
    "questions = pd.read_csv(\"C:/Users/manar/nlp project/archive/Questions.csv\",encoding=\"ISO-8859-1\")\n",
    "tags = pd.read_csv(\"C:/Users/manar/nlp project/archive/Tags.csv\",encoding=\"ISO-8859-1\", dtype={'Tag': str})\n",
    "\n",
    "questions.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n",
    "#no nan values\n",
    "\n",
    "questions['Body'] = questions['Body'].astype(str)\n",
    "questions['Title'] = questions['Title'].astype(str)\n",
    "tags['Tag'] = tags['Tag'].astype(str)\n",
    "\n",
    "questions = questions.dropna(axis=0, how=\"all\")\n",
    "tags.drop_duplicates(inplace = True)\n",
    "tags = tags.dropna(subset = ['Tag'])\n",
    "\n",
    "\n",
    "#print(tags.isna().sum())\n",
    "group_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
    "group_tags.head(5)\n",
    "group_tags.reset_index()\n",
    "group_tags.head(5)\n",
    "group_tags_final = pd.DataFrame({'Id':group_tags.index, 'Tags':group_tags.values})\n",
    "df = questions.merge(group_tags_final, on='Id')\n",
    "df.head(5)\n",
    "\n",
    "df = df[df['Score']>5]   #1 works for tf-idf  \n",
    "#get list of tags\n",
    "df['Tags'] = df['Tags'].apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "def remove_htmltags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_stopWords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    token = ToktokTokenizer()\n",
    "    words = token.tokenize(text)\n",
    "    filter_text = [word for word in words if word.casefold() not in stop_words]\n",
    "    return ' '.join(map(str, filter_text))\n",
    "\n",
    "def text_Lemmatizing(text):\n",
    "    token = ToktokTokenizer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    words = token.tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    # Remove certain types of words based on their tag\n",
    "    words = [word for word, tag in tagged_words if tag not in ['PRP', 'PRP$', 'IN']]\n",
    "    # \"v\" to put it in verb phrase\n",
    "    lemmatized_words = [lemma.lemmatize(word,pos=\"v\").lower() for word in words]\n",
    "    return ' '.join(map(str, lemmatized_words))\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def text_stemming(text):\n",
    "    # Tokenize the text\n",
    "    token = ToktokTokenizer()\n",
    "    words = token.tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    # Remove certain types of words based on their tag\n",
    "    words = [word for word, tag in tagged_words if tag not in ['PRP', 'PRP$', 'IN']]\n",
    "    # Apply Porter stemming algorithm to each word\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word).lower() for word in words]\n",
    "    \n",
    "    # Join the stemmed words back together into a string\n",
    "    return ' '.join(map(str, stemmed_words))\n",
    "\n",
    "\n",
    "def remove_specialCharacters(text):\n",
    "    # Define a string of punctuation characters\n",
    "    punct = string.punctuation  \n",
    "    # Remove all punctuation characters from the input text\n",
    "    no_punct = ''.join(char for char in text if char not in punct)   \n",
    "    return no_punct\n",
    "\n",
    "\n",
    "def preprocessing(X,root):\n",
    "    X['Body'] = X['Body'].apply(lambda x: remove_htmltags(x))\n",
    "    X['Body'] = X['Body'].apply(lambda x: remove_stopWords(x))\n",
    "    if(root == 1):\n",
    "        X['Body'] = X['Body'].apply(lambda x: text_Lemmatizing(x))\n",
    "    else:\n",
    "        X['Body'] = X['Body'].apply(lambda x: text_stemming(x))\n",
    "\n",
    "\n",
    "    X['Body'] = X['Body'].apply(lambda x: remove_specialCharacters(x))\n",
    "\n",
    "    X['Title'] = X['Title'].apply(lambda x: str(x)) \n",
    "    X['Title'] = X['Title'].apply(lambda x: remove_specialCharacters(x)) \n",
    "    X['Title'] = X['Title'].apply(lambda x: remove_stopWords(x)) \n",
    "    if(root == 1):\n",
    "        X['Title'] = X['Title'].apply(lambda x: text_Lemmatizing(x))\n",
    "    else:\n",
    "        X['Title'] = X['Title'].apply(lambda x: text_stemming(x))\n",
    "    return X\n",
    "\n",
    "# preprocessing x values , y is already preprocessed\n",
    "\n",
    "df = preprocessing(df,2)\n",
    "df['Body']\n",
    "#stemming\n",
    "\n",
    "\n",
    "#get common tags\n",
    "import nltk \n",
    "\n",
    "#if the 'Tags' column contains lists of tags for each row, this line will create a single list containing all the tags from all rows.\n",
    "all_tags = [item for sublist in df['Tags'].values for item in sublist]\n",
    "#uses the FreqDist method from nltk to create a dictionary of tag frequencies\n",
    "keywords = nltk.FreqDist(all_tags)\n",
    "\n",
    "print(keywords)\n",
    "# Get most frequent tags by creating a list called 'frequencies_words' containing the 30 most common tags and their frequencies.\n",
    "def common_tags_number(number):\n",
    "    frequencies_words = keywords.most_common(number)\n",
    "    return frequencies_words\n",
    "\n",
    "frequencies_words = common_tags_number(25)\n",
    "#creates a list called 'tags_features' containing only the tag names (not their frequencies)\n",
    "tags_features = [word[0] for word in frequencies_words]\n",
    "# Drop unnecessary columns at this point\n",
    "\n",
    "print(tags_features)\n",
    "\n",
    "def most_common_column(tags):\n",
    "    \"\"\"Function to check if tag is in most common tag list\"\"\"\n",
    "    tags_filtered = []\n",
    "    for i in range(0, len(tags)):\n",
    "        if tags[i] in tags_features:\n",
    "            tags_filtered.append(tags[i])\n",
    "    return tags_filtered\n",
    "\n",
    "# Change Tags column into None for questions that don't have a most common tag\n",
    "df['Tags'] = df['Tags'].apply(lambda x: most_common_column(x))\n",
    "#replace the empty ones to none\n",
    "df['Tags'] = df['Tags'].apply(lambda x: x if len(x)>0 else None)\n",
    "df.dropna(subset=['Tags'], inplace=True)\n",
    "print(df['Tags'].shape)\n",
    "\n",
    "df['Tags']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44282a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52418 52418 52418\n",
      "(52418, 25)\n",
      "(52418, 20000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Define X, y\n",
    "X1 = df['Body']\n",
    "X2 = df['Title']\n",
    "y = df['Tags']\n",
    "\n",
    "print(len(X1), len(X2), len(y))\n",
    "def y_encoding(y):\n",
    "    multilabel_binarizer = MultiLabelBinarizer()\n",
    "    y_bin = multilabel_binarizer.fit_transform(y)\n",
    "    return y_bin\n",
    "\n",
    "\n",
    "#feature extraction methods\n",
    "\n",
    "#Bag of words\n",
    "def Bag_of_words(x1,x2):\n",
    "    # Create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer(max_df=0.8, max_features=1000 ,stop_words='english')\n",
    "    X1_bow = vectorizer.fit_transform(x1)\n",
    "    X2_bow = vectorizer.transform(x2)\n",
    "\n",
    "    # Concatenate the two sparse matrices\n",
    "    bow_representation_x = hstack([X1_bow, X2_bow])\n",
    "    return bow_representation_x\n",
    "\n",
    "#Term Frequency-Inverse Document Frequency  DONE \n",
    "def td_idf(x1,x2):\n",
    "    vectorizer_X1 = TfidfVectorizer(max_df=0.8, max_features=10000 ,stop_words='english')\n",
    "    vectorizer_X2 = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
    "\n",
    "    X1_tfidf = vectorizer_X1.fit_transform(x1)\n",
    "    X2_tfidf = vectorizer_X2.fit_transform(x2)\n",
    "    X_tfidf = hstack([X1_tfidf,X2_tfidf])\n",
    "\n",
    "    return X_tfidf\n",
    "\n",
    "#Topic modeling (e.g. using Latent Dirichlet Allocation or Non-negative Matrix Factorization) low accuracy\n",
    "def Topic_modeling(x1,x2):\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer_X1 = TfidfVectorizer(max_df=0.8, max_features=10000 ,stop_words='english')\n",
    "    vectorizer_X2 = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
    "    \n",
    "    X1_tfidf = vectorizer_X1.fit_transform(x1)\n",
    "    X2_tfidf = vectorizer_X2.fit_transform(x2)\n",
    "    X_tfidf = hstack([X1_tfidf,X2_tfidf])\n",
    "\n",
    "    # Train an NMF model on the TF-IDF matrix\n",
    "    num_topics = 50\n",
    "    nmf_model = NMF(n_components=num_topics, random_state=0)\n",
    "    nmf_model.fit(X_tfidf)\n",
    "\n",
    "    # Get the document-topic matrix\n",
    "    X_nmf = nmf_model.transform(X_tfidf)\n",
    "    return X_nmf\n",
    "\n",
    "# Define multilabel binarizer\n",
    "y_bin = y_encoding(y)\n",
    "\n",
    "def feature_extraction(X1,X2,choice):\n",
    "    if(choice == 1):\n",
    "        X_total = td_idf(X1,X2)\n",
    "    elif(choice == 2):\n",
    "        X_total = Bag_of_words(X1,X2)\n",
    "    elif(choice == 3):\n",
    "        X_total = Word2Vec(X1,X2)\n",
    "    elif(choice ==4):\n",
    "        X_total = Topic_modeling(X1,X2)\n",
    "    return X_total\n",
    "\n",
    "X_total = feature_extraction(X1,X2,1)\n",
    "\n",
    "\n",
    "print(y_bin.shape)\n",
    "\n",
    "print(X_total.shape)\n",
    "x_train, X_test, y_train, y_test = train_test_split(X_total, y_bin, test_size = 0.2, random_state = 0)\n",
    "#X_train, X_val, Y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state= 0)\n",
    "\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Accuracy score: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Recall score: {}\".format(recall_score(y_true=y_test, y_pred=y_pred, average='weighted')))\n",
    "    \n",
    "    ''' weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n",
    "    This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "    '''\n",
    "    print(\"Precision score: {}\".format(precision_score(y_true=y_test, y_pred=y_pred, average='weighted')))\n",
    "    #Hamming-Loss is the fraction of labels that are incorrectly predicted, i.e., the fraction of the wrong labels to the total number of labels.\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
    "    print(\"F1 score: {}\".format(f1_score(y_pred, y_test, average='weighted')))\n",
    "    print(\"---\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fff4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (41934, 20000)\n",
      "X_test shape: (10484, 20000)\n",
      "y_train shape: (41934, 25)\n",
      "y_test shape: (10484, 25)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.12 GiB for an array with shape (41934, 20000) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m# initialize binary relevance multi-label classifier\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# with a gaussian naive bayes base classifier\u001b[39;00m\n\u001b[0;32m     46\u001b[0m clf \u001b[39m=\u001b[39m LabelPowerset(base_estimator)\n\u001b[1;32m---> 48\u001b[0m clf\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m     50\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     51\u001b[0m \u001b[39mprint\u001b[39m(y_pred\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\skmultilearn\\problem_transform\\lp.py:140\u001b[0m, in \u001b[0;36mLabelPowerset.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits classifier to training data\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m.. note :: Input matrices are converted to sparse format internally if a numpy representation is passed\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_input_format(\n\u001b[0;32m    138\u001b[0m     X, sparse_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m'\u001b[39m, enforce_sparse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_input_format(X),\n\u001b[0;32m    141\u001b[0m                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(y))\n\u001b[0;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.12 GiB for an array with shape (41934, 20000) and data type float32"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "print(\"X_train shape: {}\".format(x_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#we can use other classifiers as a base estimator\n",
    "#1- rfc = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "c = 2\n",
    "if(c == 1):\n",
    "    base_estimator = LinearSVC()\n",
    "elif(c == 2):\n",
    "    base_estimator = RandomForestClassifier(n_estimators=100, random_state=0)   #0.6152231972529569\n",
    "elif(c == 3):\n",
    "    base_estimator = KNeighborsClassifier(n_neighbors=5)   #0.38994463945059135\n",
    "elif(c == 4):\n",
    "    base_estimator = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0) #taking long time\n",
    "elif(c == 5):\n",
    "    base_estimator = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,solver='sgd', verbose=10, tol=1e-4, random_state=1,learning_rate_init=.1)\n",
    "\n",
    "sgd = SGDClassifier(n_jobs=-1)\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "clf = LabelPowerset(base_estimator)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print_score(y_pred, clf)\n",
    "#done for both td , bag(takes 57min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#done\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "for i in range(len(cm)):\n",
    "    fig, axs = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm[i], annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "    axs.set_title(f\"Class {i+1}\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.show()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "sns.heatmap(cm.sum(axis=0), annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"DecisionTreeClassifier Model\")\n",
    "plt.show()\n",
    "plt.savefig('confusion_matrix.jpg')\n",
    "\n",
    "\n",
    "\n",
    "'''fig, axs = plt.subplots(ncols=len(cm), figsize=(70, 40))\n",
    "\n",
    "for i in range(len(cm)):\n",
    "    sns.heatmap(cm[i], annot=True, cmap=\"Blues\", fmt=\"d\", ax=axs[i])\n",
    "    axs[i].set_title(f\"Class {i+1}\")\n",
    "    \n",
    "# Compute the confusion matrix\n",
    "sns.heatmap(cm.sum(axis=0), annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"labelset classifier Model\")\n",
    "plt.show()'''\n",
    "# Plot the confusion matrix as a heatmap\n",
    "#sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def report_formodels()\n",
    "# Compute precision, recall and F1-score for each class\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Plot a bar chart of the metrics for each class\n",
    "df[['precision', 'recall', 'f1-score']].plot(kind='bar')\n",
    "plt.title('Class-wise Metrics')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Score')\n",
    "plt.savefig('class_metrics.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A precision and recall curve shows precision and recall values at all classification thresholds. \n",
    "It summarizes the trade off between precision and recall.'''\n",
    "\n",
    "\n",
    "\n",
    "#Precision-Recall Curve:\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert numpy array to sparse matrix\n",
    "y_test_sparse = csr_matrix(y_test)\n",
    "y_pred_sparse = csr_matrix(y_pred)\n",
    "\n",
    "y_pred_dense = np.asarray(y_pred_sparse.todense())\n",
    "y_test_dense = np.asarray(y_test_sparse.todense())\n",
    "\n",
    "# Compute precision, recall and threshold values for each class separately\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "thresholds = dict()\n",
    "num_classes = 25     \n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], thresholds[i] = precision_recall_curve(y_test_dense[:, i], y_pred_dense[:, i])\n",
    "\n",
    "# Compute the average precision and recall across all classes\n",
    "avg_precision = np.mean([precision[i] for i in range(num_classes)], axis=0)\n",
    "avg_recall = np.mean([recall[i] for i in range(num_classes)], axis=0)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(avg_recall, avg_precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.savefig('precion.jpg')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
